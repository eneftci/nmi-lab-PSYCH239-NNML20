<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js â€“ NNML 2020</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/nmilab.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="lib/css/monokai.css">

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.0.0/animate.min.css"
    />
    <link rel="stylesheet" href="reveal.js-plugins/appearance/appearance.css" />

    
    <link rel="stylesheet" href="plugin/highlight/monokai.css"  id="highlight-theme"  />
<!-- Printing and PDF exports -->
<script>
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>


<script>
  //function for evaluation
  function solve()
  {
          let w0 = document.getElementById("w0").value
          let w1 = document.getElementById("w1").value
          let b = document.getElementById("b").value
          document.getElementById("a11").innerHTML = (eval(w0)*1+eval(w1)*1+eval(b)).toFixed(2)
          document.getElementById("a10").innerHTML = (eval(w0)*1+eval(w1)*0+eval(b)).toFixed(2)
          document.getElementById("a01").innerHTML = (eval(w0)*0+eval(w1)*1+eval(b)).toFixed(2)
          document.getElementById("a00").innerHTML = (eval(w0)*0+eval(w1)*0+eval(b)).toFixed(2)
          document.getElementById("y11").innerHTML = (((eval(w0)*1+eval(w1)*1+eval(b))>0)*1)
          document.getElementById("y10").innerHTML = (((eval(w0)*1+eval(w1)*0+eval(b))>0)*1)
          document.getElementById("y01").innerHTML = (((eval(w0)*0+eval(w1)*1+eval(b))>0)*1)
          document.getElementById("y00").innerHTML = (((eval(w0)*0+eval(w1)*0+eval(b))>0)*1)
          document.getElementById('w0val').innerHTML=eval(w0); 
          document.getElementById('w1val').innerHTML=eval(w1); 
          document.getElementById('bval').innerHTML=eval(b); 
        }

</script>



<script async defer src="https://buttons.github.io/buttons.js"></script>
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

        <section data-external="title.html" data-vertical-align-top data-background-color=#B2BA67 ></section>
        <section data-markdown data-vertical-align-top data-background-color=#B2BA67><textarea data-template>
            <h1> Lecture 6: Modern ConvNets for Classification and Segmentation <br/> </h1>

        </textarea></section>

<section data-markdown><textarea data-template>
    <h1> Deep Learning Goody Bag II</h1>
    <h2> Normalization, FashionMNIST, Resizing data </h2>
</textarea></section>



<section data-markdown><textarea data-template>
    <h2>Normalization</h2>
    <ul>
      <li /> If preactivations drift during training, it is costly for subsequent layers to adapt to that drift.
        <img src="images/layer_distribution.png" />
      <li /> Several normalization techniques exist, we will focus on the one use the most commonly (Batch Norm).
    </ul>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2>Batch Normalization</h2>
    <ul>

      <li /> Batch Normalization (BN) is one solution to this problem. BN transforms the activations at a given layer x according to:
    $$
  \mathrm{BN}(\mathbf{x}) = \mathbf{\gamma} \odot \frac{\mathbf{x} - \hat{\mathbf{\mu}}}{\hat\sigma} + \mathbf{\beta}</div>
    $$
    where $\hat{\mathbf{\mu}}$ is the mean over the batch, $\hat\sigma$ is the standard deviation over the batch. $\gamma$ and $\beta$ are trainable scaling and offsets parameters.
      <li class=fragment data-fragment-index="2" /> Typically a noise term is added to the calculation of $\hat\sigma$. This noise term prevents a division by zero and also acts as a regularizer
      <li class=fragment data-fragment-index="3"/> BN speeds up the training in large networks
      <li class=fragment data-fragment-index="3"/> BN is applied right before the application of the activation function
    </ul>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2>Goody II.1 Use BN in Feed Forward Neural Networks</h2>

<pre><code class="Python" data-trim data-noescape>
def __init__(self):
  super(Net, self).__init__()
  self.layer1 = torch.nn.Linear(128,64)
  self.layer1_bn =nn.BatchNorm1d(64)
  ...

def forward(self, x):
  y = self.layer1(x)
  y = self.layer1_bn(y)
  y = torch.relu(y)
  ...
</code></pre>

</textarea></section>

<section data-markdown><textarea data-template>
    <h2>Goody II.1 Use BN in Convolutional Neural Networks</h2>

<pre><code class="Python" data-trim data-noescape>
def __init__(self):
  super(Net, self).__init__()
  self.layer1 = torch.nn.Conv2d(3,32,3,1)
  self.layer1_bn =nn.BatchNorm2d(32)
  ...

def forward(self, x):
  y = self.layer1(x)
  y = self.layer1_bn(y)
  y = torch.relu(y)
  ...
</code></pre>

</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Fashion MNIST </h2>
    <ul>
      <li/> MNIST is too easy: 99.70% classification means 30 misclassified samples
      <li/> But the size is great for proof of concept. The Fashion MNIST dataset is the same size (28x28, 70k images, 10 classes) and is slightly harder
    </ul>
    <img src="images/fashion-mnist-sprite.png" class=stretch />
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Goody II.2 Use Fashion MNIST as drop-in replacement for MNIST </h2>

<ul>
<li /> Use fashion MNIST wherever you can use MNIST to test your model.
</ul>

<pre><code class="Python" data-trim data-noescape>
train_set = datasets.FashionMNIST('./data',
    train=True, download=True,
    transform=transforms.Compose([
      transforms.ToTensor(),
      transforms.Normalize((0.1307,), (0.3081,))
      ]),
    target_transform = None,)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)
</code></pre>

</textarea></section>


<section data-markdown><textarea data-template>
    <h2> Goody II.3 Data pre-processing on-the-fly </h2>
    
    <ul>
      <li /> Often, computer vision models are optimized for a given size. For example in torchvision:
        <blockquote>
          All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. 
        </blockquote>
        <li /> You can use the following transforms to make any color image compliant with this constraint:

    <pre><code class="Python" data-trim data-noescape>
    preprocess = transforms.Compose([
       transforms.Resize(256),
       transforms.CenterCrop(224),
       transforms.ToTensor(),
       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    </code></pre>
    </ul>


</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Goody II.3 Example CIFAR10 </h2>
    
    <ul>
      <li /> MNIST has only 1 channel, so you can't use it in pre-trained models (but you can modify the pre-defined models to accept a single channel)
      <li /> CIFAR10 is another commonly used dataset. You can make CIFAR10 compliant with pre-trained models as follows
    </ul>

<pre><code class="Python" data-trim data-noescape>
train_set = datasets.CIFAR10('./data',
   train=True, download=True,
   transform=preprocess,
   target_transform = None,)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)
</code></pre>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/open?id=1L0Q3f9m0xIyAx18GGhij-9rwZyQq-mYH)


</textarea></section>

<section data-markdown><textarea data-template>
    <h1> Modern Convolutional Neural Networks </h1>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> The Rise of Deep Learning </h2>
    
    <ul>
      <li /> Until 2012, neural networks were often surpassed by <em>classical</em> machine learning methods, such as support vector machines.
      <li /> Rather than training <em>end-to-end</em>, classical machine learning used hand-crafted features (SIFT, SURF, HOG).
      <li /> In the 1990s, neural network accelerators were not sufficiently powerful. ConvNets were around, but couldn't scale to large problems.
      <li /> GPU accelerators, large-scale labeled data (ImageNet) and the  perserverence of neural network researchers changed this in late 2000'
    </ ul>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> AlexNet </h2>

    <div class=row>
      <div class=column>
    <ul>
      <li /> In 2012, Krizhevsky, Sutskever, & Hinton implemented core operations (convolution operations) of CNNs on GPUs, which achieved excellent performance in the ImageNet challenge.
      <li /> The capacity of the network was contolled using dropout and data augmentation.
      <li /> Alexnet (Right) also had subtle differences with the LeNet (Left)
      <li /> In 2012, training Alexnet tooks days! Today, AlexNets are superseded by more efficient networks. 
    </ul>
      </div>
      <div class=column>
      <img src="images/alexnet.svg" class=large />
      </div>
    </div>

      <pre><code class="Python" data-trim data-noescape>
      torchvision.models.alexnet(pretrained=False)
      </code></pre>
      <p class=ref>Krizhevsky et al. 2012 "Imagenet classification with deep convolutional neural networks"</p>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Modern ConvNets: Visual Geometry Groups (VGG) </h2>
    <div class=row>
      <div class=column>
    <ul>
      <li /> VGG networks introduced the use of <em> blocks </em> that consisted of multiple layers.
      <li /> The original VGG block consists of multiple 3x3 convolution layers, a ReLU and a 2x2 max-pooling layer with stride of 2
      <li />  The first block has 64 output channels and each subsequent block doubles the number of output channels, until that number reaches 512
      <li /> The number of convolutional blocks used depended on the size of the input. The original VGG net used 5 blocks.
    </ul>
      </div>
      <div class=column>
      <img src="images/vgg.svg" class=large />
      </div>
    </div>
      <pre><code class="Python" data-trim data-noescape>
      torchvision.models.vgg16(pretrained=False)
      </code></pre>

    <p class=ref>Simonyan & Zisserman, 2014 </p>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Modern ConvNets: Network in Networks  </h2>
    <div class=row>
      <div class=column>
    <ul>
      <li /> Fully Connected (FC) layers are important to mix the spatial features for a final classification. 
      <li /> However, if their size and depth is not chosen properly, they can remove spatial structure too early.
      <li /> Networks in Networks perform mixing of channels (as in FC) while keeping the spatial structure. This is achieved using 1x1 convolutions.
      <li /> A global mean pooling layer is used to produce the output
    </ul>

    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/open?id=1ZUxCV0J-I13XV03WA-qkALeiIYoVjI-T)
      </div>
      <div class=column>
      <img src="images/nin.svg" class=large />

      <pre><code class="Python" data-trim data-noescape>
      ...
      torch.nn.Conv2d(3, 64, 5, padding=2),
      torch.nn.ReLU(inplace=True),
      torch.nn.Conv2d(64, 32, 1),
      torch.nn.ReLU(inplace=True),
      torch.nn.Conv2d(32, 32, 1),
      torch.nn.ReLU(inplace=True),
      torch.nn.MaxPool2d(3, stride=2),
      torch.nn.Dropout()
      ...
      nn.AvgPool2d(8, stride=1) #This after several nin blocks
      </code></pre>
      </div>
    </div>
    <p class=ref>Lin et al., 2013 </p>


</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Modern ConvNets: GoogLeNet </h2>
    <div class=row>
      <div class=column>
    <ul>
      <li /> Different convnets use different kernel sizes.
      <li /> GoogLeNet combined multiple convolutions with different kernel sizes in a single block.
      <li > The middle two paths perform a 1x1 convolution on the input to reduce the number of input channels, reducing the model's complexity
    </ul>
    <pre><code class="Python" data-trim data-noescape>
    torchvision.models.googlenet(pretrained=False)
    </code></pre>
      </div>
      <div class=column>
      <img src="images/inception-full.svg" class=large />
      <img src="images/inception-block.svg" class=large />
      </div>
    </div>



    <p class=ref>Szegedy et al., 2015 </p>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Modern ConvNets: ResNet </h2>
    <div class=row>
      <div class=column>
    <ul>
      <li /> A residual block attempts to output $f(x) + x$, where $f$ is the function computed by the network in the dotted box.
      <li /> In the residual block, $f(x)$ denotes the deviation from the original input 
      <li /> Standard Block (left), Residual Block (right)
      <li /> Using resnets, much deeper layers could be constructed.
    </ul>
      </div>
      <div class=column>
      <img src="images/residual-block.svg" class=large />
      </div>
    </div>

      <pre><code class="Python" data-trim data-noescape>
      torchvision.models.resnet34(pretrained=False)
      </code></pre>

    <p class=ref>He et al., 2016 </p>
</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Training a pre-defined network on your own dataset </h2>

    <ul>
      <li /> Pre-defined networks are great for getting started on a project because the structure of the network has been optimized by the community
        <li /> Note that torchvision networks are intended for ImageNet classification, <i>i.e.</i> 1000 classes. Remember to set num_classes accordingly. 
    </ul>

</textarea></section>

<section data-markdown><textarea data-template>
    <h2> In-class Assignment: Training your own model </h2>

    Let's train our own resnet18/googlenet/vgg on CIFAR10. 
    :
    <ul>
      <li /> Use the num_classes=10 argument as follows:
      <pre><code class="Python" data-trim data-noescape>
      torchvision.models.resnet18(pretrained=False, num_classes=10)
      </code></pre>
      <li /> The Resnet class works for 3 inputs channels only. How would you modify the model class? (for resnets, it is defined here: <a href=https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py> https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py </a>)
    </ul>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/open?id=1mdFea-ADcCsKTzZX7A2roqVgO5O9qsR7)

  </textarea></section>


<section data-markdown><textarea data-template>
    <h2> Semantic Segmentation</h2>
    <img src="https://miro.medium.com/max/1000/1*wbaUQkYzRhvmd7IjKJjjCg.gif" />
    <ul>
      <li /> Standard ConvNets produce one classification per image. Assuming the output of the network is an image, it should be possible to make a classification at every <em>pixel</em>.
      <li /> Using a network composed only of convolutions, is it possible to obtain an image at the end of the neural network pipeline
      <li /> Classification can then be done on every resulting pixel:

      $$ Total Loss = \sum_{ij} Loss(Y_{ij},T_{ij}) $$

    </ul>



</textarea></section>

<section data-markdown><textarea data-template>
    <h2> <s>Deconvolution</s> Transposed Convolution </h2>
    <ul>
      <li /> The output of a convolutional network has fewer pixels. Additional layers can be used to remap the classification to the original image.
      <li /> Convolution operations can be reversed. In fact this is what backpropagation in convolutional layers do
      <li /> Convolutional layers with stride 2 downsample by a factor 2 on each axis. Transposed convolutions do the opposite (a stride of 1/2 upsamples the image by 2).
      <li /> Contrary to standard upsampling (e.g. bilinear), the backward convolution has learnable parameters.
    </ul>
    <img src=images/deconv.gif class=small />
    <a href="https://github.com/vdumoulin/conv_arithmetic"> more animations here </a>


</textarea></section>

<section data-markdown><textarea data-template>
    <h2> Fully Convolutional Networks for Semantic Segmentation </h2>
    <div class=row>
      <div class=column>
      Without upsampling layer
      <img src="images/fcn_conv.png" />
      </div>
      <div class=column>
      With upsampling layer
      <img src="images/fcn_semantic_segmentation.png" />
      </div>
    </div>
    <p class=ref>Long et al. 2015</p>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/open?id=1EFTCtTz4swVicOf114o_PTe37lOm6eJa)

</textarea></section>

<section data-markdown><textarea data-template>
    <h2> U-Nets </h2>
    <div class=row>
      <div class=column>
      U-Net architecture
      <img src="images/unet_arch.png" class=large/>
      </div>
      <div class=column>
      With upsampling layer
      <img src="images/unet_example.png" class=large/>
      </div>
    </div>
    <p class=ref><a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28">Ronneberger et al. 2015</a></p>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/open?id=1EFTCtTz4swVicOf114o_PTe37lOm6eJa)

</textarea></section>

<!--
<section data-markdown><textarea data-template>
    <h2>Analyzing Neural Networks</h2>
    <ul>
      <li/>Patterns that maximally activate the neuron roughly correspond to a local maximum of the tuning curve.
      <li/>To find it, one can maximize activity by gradient descent.
      $$ \hat{x} = \mathrm{argmax}_x y_i(x)  $$
      <li /> In practice, this doesn't produce nice images, but "optical illusions" of neural networks. The optimization must be regularized or a prior must be applied to obtain meaningful images.
</textarea></section>


<section data-markdown><textarea data-template>
    <h2>Analyzing Neural Networks: Which objective? </h2>

    <img src="images/visualization_optimization_objective.png" />
    <p class=ref> Olah et al. 2017, https://distill.pub/2017/feature-visualization/ </p>

</textarea></section>

<section data-markdown><textarea data-template>
    <h2>Analyzing Neural Networks: What does a neural network respond to? </h2>

    <img src="images/visualization_class.png" />
    <p class=ref> Olah et al. 2017, https://distill.pub/2017/feature-visualization/ </p>

</textarea></section>

<section data-markdown><textarea data-template>
      <h2>Deep Dream</h2>
<iframe width="794" height="547" src="https://www.youtube.com/embed/SCE-QeDfXtA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <p class=ref> Google Blog, June 2015</p>

      [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/open?id=1Uy0nWFaoNQ-QuLIH8-ANp2NMpo_WFNBU)
  </textarea></section> --!>

<section data-markdown><textarea data-template>
      <h2>Machine Behavior - Cognitive Sciences of the Future?</h2>
      <div class=row> 
      <div class=column> 
      <img src="images/machine_behavior_first.png" style="max-height:500px" />
      </div>
      <div class=column> 
      <img src="images/machine_behavior_fig1.png" style="max-height:500px" />
      </div>
      </div>
      <a href=https://www.nature.com/articles/s41586-019-1138-y?error=cookies_not_supported&code=113da7cb-dcd8-44c3-b755-c33b6fec01fe>https://www.nature.com/articles/s41586-019-1138-y</a>
      <p class=pl>Discusson on Week 6: Please read!</p>
  </textarea></section>


      </div>
    </div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
    <script src="../reveal.js-plugins/menu/menu.js"></script>
    <script src="reveal.js-plugins/audio-slideshow/plugin.js"></script>
    <script src="reveal.js-plugins/audio-slideshow/recorder.js"></script>
    <script src="reveal.js-plugins/audio-slideshow/RecordRTC.js"></script>
    <script src="reveal.js-plugins/chalkboard/plugin.js"></script>
    <script src="reveal.js-plugins/external/external.js"></script>
    <script src="reveal.js-plugins/externalcode/externalcode.js"></script>
    <script src="reveal.js-plugins/appearance/appearance.js"></script>

		<script>
      Reveal.configure({ pdfMaxPagesPerSlide: 1, hash: true, slideNumber: true})
			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
        mouseWheel: false,
        width: 1280,
        height: 720,
        margin: 0.0,
        navigationMode: 'grid',
        transition: 'fade',
				controls: true,
				progress: true,
				center: true,
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [Appearance, RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealAudioRecorder, RevealAudioSlideshow, RevealMath, RevealExternal, Externalcode, RevealMenu],

      	menu: { // Menu works best with font-awesome installed: sudo apt-get install fonts-font-awesome
					themes: false,
					transitions: false,
					markers: true,
					hideMissingTitles: true,
					custom: [
				            { title: 'Plugins', icon: '<i class="fa fa-external-link-alt"></i>', src: 'toc.html' },
				            { title: 'About', icon: '<i class="fa fa-info"></i>', src: 'about.html' }
				        ]
				},

      audio: {
        prefix: 'audio/lecture_6/', 	// audio files are stored in the "audio" folder
        suffix: '.ogg',		// audio files have the ".ogg" ending
        textToSpeechURL: null,  // the URL to the text to speech converter
        defaultNotes: false, 	// use slide notes as default for the text to speech converter
        defaultText: false, 	// use slide text as default for the text to speech converter
        advance: 300, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance
        autoplay: false,	// automatically start slideshow
        defaultDuration: 1.0,	// default duration in seconds if no audio is available
        defaultAudios: true,	// try to play audios with names such as audio/1.2.ogg
        playerOpacity: 0.5,	// opacity value of audio player if unfocused
        playerStyle: 'position: fixed; bottom: 4px; left: 25%; width: 50%; height:75px; z-index: 33;', // style used for container of audio controls
        startAtFragment: false, // when moving to a slide, start at the current fragment or at the start of the slide
      },
      // ...


				keyboard: { 
					82: function() { Recorder.toggleRecording(); },	// press 'r' to start/stop recording
					90: function() { Recorder.downloadZip(); }, 	// press 'z' to download zip containing audio files
					84: function() { Recorder.fetchTTS(); } 	// press 't' to fetch TTS audio files
  				}


    });
		</script>


	</body>
</html>
